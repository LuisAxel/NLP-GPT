{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "NLyR9luRWsxI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# The MIT License (MIT) Copyright (c) 2023 Emilio Morales\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
        "# this software and associated documentation files (the \"Software\"), to deal in the Software without\n",
        "# restriction, including without limitation the rights to use, copy, modify, merge, publish,\n",
        "# distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in all copies or\n",
        "# substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n",
        "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
        "# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES\n",
        "# OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
        "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqusX_h4WsxK"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/LuisAxel/NLP-GPT/blob/main/GPT.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty7Z3RRm2v0P"
      },
      "source": [
        "# NLP Programa 3: GPT  \n",
        "-------\n",
        "Integrantes:\n",
        "- Andrés Urbano Andrea\n",
        "- Núñez Quintana Luis Axel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGzYYO4GrpEf"
      },
      "source": [
        "## 0.- Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N9pDYARr4pJ",
        "outputId": "f4f2b9cb-9d2f-447a-d6dc-f72c241251f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_core in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_core) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras_core) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_core) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_core) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_core) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "fb9vrk3RrpEh"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import keras_core as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import random\n",
        "from sklearn.decomposition import PCA\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import vocab as Vocab\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "Op_pMNPsrpEj"
      },
      "outputs": [],
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LYOEtsorpEk",
        "outputId": "7bf8ed9b-c95c-4748-c6bb-ea3d02aa8124"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c15968745d0>"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "torch.__version__\n",
        "torch.manual_seed(77)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "b6xe02ierpEl"
      },
      "outputs": [],
      "source": [
        "# Disable warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuZuE99sWsxO"
      },
      "source": [
        "## 1.- Conjuntos de entrenamiento y validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "dnlt2x--WsxV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def download_text_pairs():\n",
        "    path_to_zip = tf.keras.utils.get_file(\n",
        "        'spa-eng.zip',\n",
        "        origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "        extract=True)\n",
        "    path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'\n",
        "\n",
        "    with open(path_to_file) as f:\n",
        "        lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "    text_pairs = []\n",
        "    for line in lines:\n",
        "        eng, spa = line.lower().split(\"\\t\")\n",
        "        text_pairs.append((eng, spa))\n",
        "    return text_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "kwShSolerpEr"
      },
      "outputs": [],
      "source": [
        "def split_text_pairs(text_pairs, val_percentage = 0.005, random_seed=43):\n",
        "    random.Random(random_seed).shuffle(text_pairs)\n",
        "    num_val_samples = int(val_percentage * len(text_pairs))\n",
        "    num_train_samples = len(text_pairs) - num_val_samples\n",
        "    train_pairs = text_pairs[:num_train_samples]\n",
        "    val_pairs = text_pairs[num_train_samples:]\n",
        "    return train_pairs, val_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "Xunx2ixcrpEs"
      },
      "outputs": [],
      "source": [
        "def merge_pairs(text_pairs):\n",
        "    return [eng + ' ' + spa for eng, spa in text_pairs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R58aq1ZtrpEt",
        "outputId": "c08d6ff8-a229-4b39-d274-267b370e9ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "118370 training pairs\n",
            "594 validation pairs\n",
            "the old woman fell and could not get up. la anciana se cayó y no pudo levantarse.\n",
            "what is this the abbreviation for? ¿de qué es abreviatura esto?\n",
            "you're not sick. no estás enferma.\n"
          ]
        }
      ],
      "source": [
        "text_pairs = download_text_pairs()\n",
        "train_pairs, val_pairs = split_text_pairs(text_pairs)\n",
        "test_pairs = val_pairs\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "\n",
        "train_pairs = merge_pairs(train_pairs)\n",
        "val_pairs = merge_pairs(val_pairs)\n",
        "\n",
        "for s in train_pairs[:3]:\n",
        "    print(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4KjIrLz8Ryf",
        "outputId": "f4ace4f8-ec84-4556-cba5-87748562de3c"
      },
      "source": [
        "## 2.- Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWW4RxWhpAZf"
      },
      "source": [
        "- Crea vocabulario y define tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVOkPyAPr0H7",
        "outputId": "b4818935-9e8b-40c4-9026-fbc076e8adcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "0viwLWSUpGrk"
      },
      "outputs": [],
      "source": [
        "spa_tokenizer = get_tokenizer('spacy', language='es_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "qpTS_ktRo_xd"
      },
      "outputs": [],
      "source": [
        "def build_vocab(text, tokenizer):\n",
        "    counter = Counter()\n",
        "    for string_ in text:\n",
        "        counter.update(tokenizer(string_))\n",
        "    return Vocab(counter, specials=['<unk>', '<pad>', '<eos>', '<bos>'])\n",
        "\n",
        "spa_vocab = build_vocab(train_pairs + val_pairs, spa_tokenizer)\n",
        "spa_vocab.set_default_index(37546) # evita error <ukn>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlV0maHzt88t",
        "outputId": "6d1bbcfc-9ac8-4285-9119-26d39341b251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab sizes: Spanish - 38433\n"
          ]
        }
      ],
      "source": [
        "spa_vocab_size = len(spa_vocab)\n",
        "print(f'Vocab sizes: Spanish - {spa_vocab_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "Og1BFvJrWsxa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "maxlen = 64\n",
        "\n",
        "def data_process(text, vocab, tokenizer):\n",
        "    data = []\n",
        "    for raw_txt in text:\n",
        "        tensor_ = torch.tensor([vocab[token] for token in tokenizer(raw_txt)],\n",
        "                                dtype=torch.long)\n",
        "        if tensor_.shape[0] < maxlen - 2: #We are adding bos and eos\n",
        "            x = tensor_[:]\n",
        "            y = tensor_[:]\n",
        "            data.append((x, y))\n",
        "    return data\n",
        "\n",
        "train_data = data_process(train_pairs, spa_vocab, spa_tokenizer)\n",
        "val_data = data_process(val_pairs, spa_vocab, spa_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGhY3VWErpEw",
        "outputId": "fc6e4674-1039-459b-ea10-d0a70f6986c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data size: 118352, val data size: 594\n"
          ]
        }
      ],
      "source": [
        "print(f'train data size: {len(train_data)}, val data size: {len(val_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "cW6E9jkJTe1o"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "PAD_IDX = spa_vocab['<pad>']\n",
        "EOS_IDX = spa_vocab['<eos>']\n",
        "BOS_IDX = spa_vocab['<bos>']\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "    x, y = [], []\n",
        "    for (x_item, y_item) in data_batch:\n",
        "        x.append(torch.cat([torch.tensor([BOS_IDX]), x_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "        y.append(torch.cat([y_item, torch.tensor([EOS_IDX]), torch.tensor([PAD_IDX])], dim=0))\n",
        "\n",
        "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
        "    y = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
        "                          shuffle=True, collate_fn=generate_batch,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size,\n",
        "                        shuffle=True, collate_fn=generate_batch,\n",
        "                        num_workers=4, pin_memory=True)\n",
        "\n",
        "test_loader = DataLoader(val_data, batch_size=batch_size,\n",
        "                        shuffle=True, collate_fn=generate_batch,\n",
        "                        num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_N6IRYsT4ry",
        "outputId": "4d76f862-cacd-4596-a5a8-3b4a48242665",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.06 s ± 320 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "train_batch, target_batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "aauQX2tFXD8K"
      },
      "outputs": [],
      "source": [
        "train_batch, target_batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROw_9EjsT2Et",
        "outputId": "ae2e7e2b-0cf7-4b0b-8705-f3de5713956a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 32]), torch.Size([128, 32]))"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "train_batch.shape, target_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUkNHgxYrpEx",
        "outputId": "5ebfbd47-a2cf-44f7-c82d-c8a7a052e0fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([    3,   157,    23, 19349,   540,    41,     4,   197,  6072,    13,\n",
              "           157,   159, 19350,   321,   123,   203,  6074,    13,     2,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1]),\n",
              " tensor([  157,    23, 19349,   540,    41,     4,   197,  6072,    13,   157,\n",
              "           159, 19350,   321,   123,   203,  6074,    13,     2,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1]))"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "train_batch[0], target_batch[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ7f4DHJreIj"
      },
      "source": [
        "## 3.- Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRwHxAuGYEKa",
        "outputId": "45b4ab53-d5f0-4d22-fb45-794c766d3b6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039],\n",
              "         [-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039],\n",
              "         [-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039],\n",
              "         ...,\n",
              "         [-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039],\n",
              "         [-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039],\n",
              "         [-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, kv_dim, q_dim, maxlen, n_heads=4, bias=True):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.scale = (kv_dim // n_heads) ** -0.5       # 1/sqrt(d)\n",
        "        self.q = nn.Linear(q_dim, q_dim, bias = bias)\n",
        "        self.k = nn.Linear(kv_dim, q_dim, bias = bias)\n",
        "        self.v = nn.Linear(kv_dim, q_dim, bias = bias)\n",
        "\n",
        "        self.o = nn.Linear(q_dim, q_dim, bias = bias)\n",
        "\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
        "\n",
        "    def forward(self, kv, q):\n",
        "        B, L_kv, D_kv = kv.shape\n",
        "        B, L_q,  D_q = q.shape\n",
        "\n",
        "        q = self.q(q)\n",
        "        k = self.k(kv)\n",
        "        v = self.v(kv)\n",
        "\n",
        "\n",
        "        q = torch.reshape(q, [B, L_q, self.n_heads, -1])     # B, L_q,  nh,  i\n",
        "        q = torch.permute(q, [0, 2, 1, 3])                   # B, nh,   L_q, i\n",
        "\n",
        "        k = torch.reshape(k, [B, L_kv, self.n_heads, -1])    # B, L_kv, nh,  i\n",
        "        k = torch.permute(k, [0, 2, 3, 1])                   # B, nh,   i,   L_kv\n",
        "\n",
        "        v = torch.reshape(v, [B, L_kv, self.n_heads, -1])    # B, L_kv, nh,   i\n",
        "        v = torch.permute(v, [0, 2, 1, 3])                   # B, nh,   L_kv, i\n",
        "\n",
        "        qk = torch.matmul(q, k) * self.scale                 #(B, nh, L_q, i)(B, nh, i, L_kv)\n",
        "                                                             # B, nh, L_q, L_kv\n",
        "\n",
        "        # Preguntar profesor\n",
        "        qk = qk.masked_fill(self.bias[:,:,:L,:L] == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(qk, dim=-1)\n",
        "\n",
        "        v_attn = torch.matmul(attn, v)                       #(B, nh, L_q, L_kv)(B, nh, L_kv, i)\n",
        "                                                             # B, nh, L_q, i\n",
        "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])         # B, L_q, nh, i\n",
        "        v_attn = torch.reshape(v_attn, [B, L_q, D_q])        # B, L_q, D_q\n",
        "\n",
        "        x = self.o(v_attn)\n",
        "        return x\n",
        "\n",
        "\n",
        "test_layer = Attention(32, maxlen, n_heads=1)\n",
        "test_layer(torch.ones([1, maxlen, 32]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJmNuwH1clQd",
        "outputId": "e0b63759-0066-4378-8cc9-2a1d8842cb18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, kv_dim, q_dim, maxlen, heads=4, mlp_dim=512, rate=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln_kv = nn.LayerNorm(kv_dim)\n",
        "        self.ln_q = nn.LayerNorm(q_dim)\n",
        "\n",
        "        self.attn = Attention(kv_dim, q_dim, maxlen)\n",
        "\n",
        "        self.ln_2 = nn.LayerNorm(q_dim)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(rate),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(rate),\n",
        "        )\n",
        "\n",
        "    def forward(self, kv, q):\n",
        "        x = self.attn(self.ln_kv(kv), self.ln_q(q)) + q\n",
        "        return self.mlp(self.ln_2(x)) + x\n",
        "\n",
        "test_layer = Transformer(32, maxlen)\n",
        "test_layer(torch.ones([1, maxlen, 32])).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJaYcsx0ef58",
        "outputId": "21553690-2be7-416e-e132-75911d3ea22b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "train_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2ndlTvDWsxf",
        "outputId": "d90e3703-5be2-4db1-92d8-002260f96ab4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 32, 38433]), torch.Size([128, 32]))"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim, num_latents, vocab_size, maxlen, depth=3,\n",
        "                 mlp_dim=512, rate=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, inptut_dim)\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, maxlen, input_dim))\n",
        "\n",
        "        self.cross_attn = Transformer(input_dim, latent_dim, maxlen)\n",
        "\n",
        "        self.transformer = nn.Sequential()\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.transformer.append(Transformer(latent_dim, latent_dim, maxlen))\n",
        "\n",
        "        self.head = nn.Linear(latent_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        x = self.embedding(x)\n",
        "        x += self.pos_embedding[:, :L]\n",
        "        x = self.cross_attn(kv = x, q = self.latent.repeat(B, 1, 1))\n",
        "        x = self.transformer(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_dim = 128\n",
        "depth = 3\n",
        "mlp_dim = 128\n",
        "\n",
        "gpt = GPT(input_dim=model_dim, latent_dim = 128, num_latents = 512, vocab_size=spa_vocab_size,\n",
        "          maxlen=maxlen, depth=depth, mlp_dim=mlp_dim)\n",
        "output = gpt(train_batch)\n",
        "output.shape, target_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsJbLyZ6b_57"
      },
      "source": [
        "## 4.- Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gxX1q5GrpE0",
        "outputId": "abbc5501-36c3-4bfc-d873-98963749ee8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (embedding): Embedding(38433, 128)\n",
              "  (transformer): Sequential(\n",
              "    (0): Transformer(\n",
              "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (k): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (v): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (o): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (4): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): Transformer(\n",
              "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (k): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (v): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (o): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (4): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): Transformer(\n",
              "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (k): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (v): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (o): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (4): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Linear(in_features=128, out_features=38433, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etio6ZTwrpE0",
        "outputId": "23d2c712-fd51-43df-960e-1ff6876fd220"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ],
      "source": [
        "PAD_IDX = spa_vocab.get_stoi()['<pad>']\n",
        "EOS_IDX = spa_vocab.get_stoi()['<eos>']\n",
        "BOS_IDX = spa_vocab.get_stoi()['<bos>']\n",
        "\n",
        "PAD_IDX, EOS_IDX, BOS_IDX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "ZyJNkKRkrpE1"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(gpt.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "R5bmAldOcJn0"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    start = time.time()\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        targets = targets.view(-1)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwUH9ZlHWsxh",
        "outputId": "d760cf0d-5e49-4887-8ffd-590485e22f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "i hate mondays . norm navideños galleta chair judgements pagaba pesqué millenia provocación jefa banned escolares pitch laughter tercero calma overheard pals ladraba negociarán hourglass negociaciones empezando enseñe reexpedirme pidieron submerged oir llamadas párense interpreté acosar drummer chuleta enmendar buffet coal sonreíste cenar bolearon denme serte fleece juzgado promueve sonrisa refería payment murais ondeaban ofrecerte mecanógrafo judío clenched anonadado archaeologists canoa weaker cumpliera temerle\n",
            "\n",
            "i love my cat . estuvieras asuntos consejero mouths earache cuidarme explicármelo dressing changed raking witnessed complace speculating museums vendó exageré thin ladraba negociarán hourglass pregúntenle advertirle cargara lame crueldad neumonitis surte adjuntando vas nevadas poste parranda bombear suggestion ofenderte vuestro zapatos pierdes bolearon denme serte fleece hunde compañeros enmarqué trepó sacudirse mocoso nadamos agosto corta compartían asesinado causé antipático asfixian firmada triatlón snore\n",
            "\n",
            "i like apples . norm perjudicará calamidad preguntarán concealed pour relojes solamente cliche antártida grass purred sobres hijas derrotar escobas piel techno astronomer sue suceder arteria travel ruled bury singapore accounting finjamos pachanga recordaran language notaste freezing skating hurgó examenes despedí refugee extraordinariio avísele pública tilín reciprocated discoteca luxuries medianamente honesto pájaros niña promptly autocorrigió celebraste luck significant martians forklift aman postrado melón cometidos\n"
          ]
        }
      ],
      "source": [
        "def translate(model, sentence, device, maxlen, vocab, tokenizer):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        idx = torch.tensor([vocab[token] for token in tokenizer(sentence)],\n",
        "                                    dtype=torch.long)\n",
        "        idx = idx.reshape([1, -1])\n",
        "        maxlen = maxlen - idx.shape[-1]\n",
        "\n",
        "        for _ in range(maxlen):\n",
        "            idx = idx.to(device)\n",
        "            logits = gpt(idx)[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        txt = ' '.join([vocab.get_itos()[_] for _ in idx[0]])\n",
        "\n",
        "    # Cut generation until <eos>\n",
        "    return txt.split(\"<eos>\")[0]\n",
        "\n",
        "sentences = ['i hate mondays.',\n",
        "             'i love my cat.',\n",
        "             'i like apples.']\n",
        "\n",
        "for s in sentences:\n",
        "    trans = translate(gpt, s, device, maxlen, spa_vocab, spa_tokenizer)\n",
        "    print(f\"\\n{trans}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "EvsuJHMBlm9y",
        "outputId": "786c30b8-c3f9-4fb2-8533-8e3dfb5854c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Time for epoch 0 is 77.062866 sec Train loss: 4.346856\n",
            "i hate mondays . odio . \n",
            "i love my cat . lo hice que no le encanta el gato . \n",
            "i like apples . me gusta el otro . \n",
            "\n",
            "Time for epoch 1 is 77.820598 sec Train loss: 3.132414\n",
            "i hate mondays . odio . \n",
            "i love my cat . mi gato me gusta . \n",
            "i like apples . me gusta el manzanas . \n",
            "\n",
            "Time for epoch 2 is 76.522387 sec Train loss: 2.686997\n",
            "i hate mondays . odias el agua . \n",
            "i love my cat . mi gato . \n",
            "i like apples . me gustan los manzanas . \n",
            "\n",
            "Time for epoch 3 is 78.103137 sec Train loss: 2.428159\n",
            "i hate mondays . odio el lunes . \n",
            "i love my cat . mi gato . \n",
            "i like apples . me gusta el manzanas . \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-18b152d83b9f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Translate test sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-987b5627fb17>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 6\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(gpt, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    # Translate test sentences\n",
        "    for s in sentences:\n",
        "        trans = translate(gpt, s, device, maxlen, spa_vocab, spa_tokenizer)\n",
        "        print(trans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYVY8st5rpE3"
      },
      "source": [
        "## 5.- Evaluación (BLEU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEQXDFwArpE3"
      },
      "outputs": [],
      "source": [
        "def bleu_example():\n",
        "    # Lista de oraciones de referencia (lista de listas)\n",
        "    referencias = [['El', 'gato', 'está', 'en', 'la', 'alfombra'],\n",
        "                   ['El', 'perro', 'juega', 'en', 'el', 'parque'],\n",
        "                   ['El', 'cielo', 'está', 'despejado'],\n",
        "                   ['El', 'sol', 'brilla', 'intensamente'],\n",
        "                   ['Los', 'pájaros', 'cantan', 'en', 'los', 'árboles']]\n",
        "\n",
        "    # Lista de oraciones candidatas (lista de listas)\n",
        "    candidatas = [['El', 'gato', 'está', 'durmiendo', 'en', 'la', 'alfombra'],\n",
        "                  ['El', 'perro', 'juega', 'en', 'el', 'jardín'],\n",
        "                  ['El', 'cielo', 'está', 'soleado'],\n",
        "                  ['El', 'sol', 'brilla', 'intensamente'],\n",
        "                  ['Los', 'pájaros', 'trinan', 'en', 'los', 'árboles']]\n",
        "\n",
        "    # Calcular el BLEU score para cada oración candidata\n",
        "    for i in range(len(candidatas)):\n",
        "        referencia = referencias[i]\n",
        "        candidata = candidatas[i]\n",
        "\n",
        "        bleu_score = nltk.translate.bleu_score.sentence_bleu([referencia], candidata)\n",
        "        print(f\"BLEU score para la oración {i+1}: {bleu_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG2Dyu3grpE3"
      },
      "outputs": [],
      "source": [
        "bleu_example()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8EiOMqLDeYr"
      },
      "outputs": [],
      "source": [
        "def format_string(s):\n",
        "  # Remove special characters\n",
        "  s = s.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "  # Delete multiple spaces\n",
        "  return ' '.join(s.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szj1creo-AC9"
      },
      "outputs": [],
      "source": [
        "def bleu_eval(test_data):\n",
        "\n",
        "  # Divide pairs in input/target\n",
        "  input = [s for s, _ in test_data]\n",
        "  target = [t for _, t in test_data]\n",
        "\n",
        "  # Get model outputs\n",
        "  output = []\n",
        "  for s in input:\n",
        "    trans = translate(gpt, s, device, maxlen, spa_vocab, spa_tokenizer)\n",
        "    output.append(trans)\n",
        "\n",
        "  # Delete multiple spaces and special characters\n",
        "  input = [format_string(s) for s in input]\n",
        "  target = [format_string(s) for s in target]\n",
        "  output = [format_string(s) for s in output]\n",
        "\n",
        "  # Delete input part from model output\n",
        "  for i in range(0, len(output)):\n",
        "    output[i] = output[i][len(input[i]) + 1:]\n",
        "\n",
        "  # Make list of lists for BLEU\n",
        "  target = [s.split() for s in target]\n",
        "  output = [s.split() for s in output]\n",
        "\n",
        "  # Compute BLEU\n",
        "  score = 0\n",
        "  for i in range(len(output)):\n",
        "    t = target[i]\n",
        "    o = output[i]\n",
        "\n",
        "    bleu_score = nltk.translate.bleu_score.sentence_bleu([t], o)\n",
        "    score += bleu_score\n",
        "\n",
        "  print(f'BLEU score promedio: {score/len(output)}')\n",
        "\n",
        "bleu_eval(test_pairs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}