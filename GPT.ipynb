{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NLyR9luRWsxI"
   },
   "outputs": [],
   "source": [
    "# The MIT License (MIT) Copyright (c) 2023 Emilio Morales\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in the Software without\n",
    "# restriction, including without limitation the rights to use, copy, modify, merge, publish,\n",
    "# distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or\n",
    "# substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n",
    "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
    "# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES\n",
    "# OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqusX_h4WsxK"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/LuisAxel/NLP-GPT/blob/main/GPT.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty7Z3RRm2v0P"
   },
   "source": [
    "# NLP Programa 3: GPT  \n",
    "-------\n",
    "Integrantes:\n",
    "- Andrés Urbano Andrea\n",
    "- Núñez Quintana Luis Axel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGzYYO4GrpEf"
   },
   "source": [
    "## 0.- Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_N9pDYARr4pJ",
    "outputId": "fbdc6a9e-e87a-49f5-99c0-0742be2e945b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_core in ./env/lib/python3.10/site-packages (0.1.7)\n",
      "Requirement already satisfied: rich in ./env/lib/python3.10/site-packages (from keras_core) (13.7.1)\n",
      "Requirement already satisfied: namex in ./env/lib/python3.10/site-packages (from keras_core) (0.0.8)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.10/site-packages (from keras_core) (1.26.4)\n",
      "Requirement already satisfied: dm-tree in ./env/lib/python3.10/site-packages (from keras_core) (0.1.8)\n",
      "Requirement already satisfied: absl-py in ./env/lib/python3.10/site-packages (from keras_core) (2.1.0)\n",
      "Requirement already satisfied: h5py in ./env/lib/python3.10/site-packages (from keras_core) (3.11.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./env/lib/python3.10/site-packages (from rich->keras_core) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./env/lib/python3.10/site-packages (from rich->keras_core) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras_core) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fb9vrk3RrpEh"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import keras_core as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Op_pMNPsrpEj"
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LYOEtsorpEk",
    "outputId": "60719408-0969-4c6e-a2b7-8145e3977ff0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7958453d3fd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "torch.manual_seed(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b6xe02ierpEl"
   },
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuZuE99sWsxO"
   },
   "source": [
    "## 1.- Conjuntos de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dnlt2x--WsxV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_text_pairs():\n",
    "    path_to_zip = tf.keras.utils.get_file(\n",
    "        'spa-eng.zip',\n",
    "        origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "        extract=True)\n",
    "    path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "    text_pairs = []\n",
    "    for line in lines:\n",
    "        eng, spa = line.lower().split(\"\\t\")\n",
    "        text_pairs.append((eng, spa))\n",
    "    return text_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kwShSolerpEr"
   },
   "outputs": [],
   "source": [
    "def split_text_pairs(text_pairs, val_percentage = 0.005, random_seed=43):\n",
    "    random.Random(random_seed).shuffle(text_pairs)\n",
    "    num_val_samples = int(val_percentage * len(text_pairs))\n",
    "    num_train_samples = len(text_pairs) - num_val_samples\n",
    "    train_pairs = text_pairs[:num_train_samples]\n",
    "    val_pairs = text_pairs[num_train_samples:]\n",
    "    return train_pairs, val_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Xunx2ixcrpEs"
   },
   "outputs": [],
   "source": [
    "def merge_pairs(text_pairs):\n",
    "    return [eng + ' ' + spa + ' <eos>'for eng, spa in text_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R58aq1ZtrpEt",
    "outputId": "17a54ba8-f6b9-4d02-d809-1ab02e87e0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "118370 training pairs\n",
      "594 validation pairs\n",
      "the old woman fell and could not get up. la anciana se cayó y no pudo levantarse. <eos>\n",
      "what is this the abbreviation for? ¿de qué es abreviatura esto? <eos>\n",
      "you're not sick. no estás enferma. <eos>\n"
     ]
    }
   ],
   "source": [
    "text_pairs = download_text_pairs()\n",
    "train_pairs, val_pairs = split_text_pairs(text_pairs)\n",
    "test_pairs = val_pairs\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "\n",
    "train_pairs = merge_pairs(train_pairs)\n",
    "val_pairs = merge_pairs(val_pairs)\n",
    "\n",
    "for s in train_pairs[:3]:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4KjIrLz8Ryf",
    "outputId": "f4ace4f8-ec84-4556-cba5-87748562de3c"
   },
   "source": [
    "## 2.- Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWW4RxWhpAZf"
   },
   "source": [
    "- Crea vocabulario y define tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVOkPyAPr0H7",
    "outputId": "573e3a36-c10b-4356-d74a-4db10c9cb659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in ./env/lib/python3.10/site-packages (from es-core-news-sm==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.0)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (59.6.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.7.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: language-data>=1.2 in ./env/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in ./env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.11.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./env/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./env/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./env/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./env/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in ./env/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0viwLWSUpGrk"
   },
   "outputs": [],
   "source": [
    "#eng_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "spa_tokenizer = get_tokenizer('spacy', language='es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qpTS_ktRo_xd"
   },
   "outputs": [],
   "source": [
    "def build_vocab(text, tokenizer):\n",
    "    counter = Counter()\n",
    "    for string_ in text:\n",
    "        counter.update(tokenizer(string_))\n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<eos>'])\n",
    "\n",
    "spa_vocab = build_vocab(train_pairs + val_pairs, spa_tokenizer)\n",
    "spa_vocab.set_default_index(37546) # evita error <ukn>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlV0maHzt88t",
    "outputId": "3b25e2ce-ea85-42f6-d467-0c99f55241d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab sizes: Spanish - 38435\n"
     ]
    }
   ],
   "source": [
    "spa_vocab_size = len(spa_vocab)\n",
    "print(f'Vocab sizes: Spanish - {spa_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Og1BFvJrWsxa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxlen = 64\n",
    "\n",
    "def data_process(text, vocab, tokenizer):\n",
    "    data = []\n",
    "    for raw_txt in text:\n",
    "        tensor_ = torch.tensor([vocab[token] for token in tokenizer(raw_txt)],\n",
    "                                dtype=torch.long)\n",
    "        if tensor_.shape[0] < maxlen:\n",
    "            x = tensor_[:-1]\n",
    "            y = tensor_[1:]\n",
    "            data.append((x, y))\n",
    "    return data\n",
    "\n",
    "train_data = data_process(train_pairs, spa_vocab, spa_tokenizer)\n",
    "val_data = data_process(val_pairs, spa_vocab, spa_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGhY3VWErpEw",
    "outputId": "9e936305-4aa0-46dc-eddb-f207c416c3d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 118350, val data size: 594\n"
     ]
    }
   ],
   "source": [
    "print(f'train data size: {len(train_data)}, val data size: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cW6E9jkJTe1o"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "PAD_IDX = spa_vocab['<pad>']\n",
    "EOS_IDX = spa_vocab['<eos>']\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    x, y = [], []\n",
    "    for (x_item, y_item) in data_batch:\n",
    "        x.append(torch.cat([x_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        y.append(torch.cat([y_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
    "    y = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
    "                          shuffle=True, collate_fn=generate_batch,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch,\n",
    "                        num_workers=4, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(val_data, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch,\n",
    "                        num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_N6IRYsT4ry",
    "outputId": "8449d59a-86c4-4b93-f669-42a98cf9b6ca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.79 s ± 113 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aauQX2tFXD8K"
   },
   "outputs": [],
   "source": [
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROw_9EjsT2Et",
    "outputId": "ddbae94c-059e-46a9-cc0a-b53dbe19ee5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 36]), torch.Size([128, 36]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUkNHgxYrpEx",
    "outputId": "3676ea23-28df-4109-ebfd-f5fad5da9364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1002,   90,   83, 1484,  190,   83,   41,   60,  429,   29,   30,   61,\n",
       "          32,  800, 3052,  559,  470,  435,   29,   21,   22,    2,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ7f4DHJreIj"
   },
   "source": [
    "## 3.- Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRwHxAuGYEKa",
    "outputId": "d0441d72-1d96-4288-fd17-c59558a67010"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039],\n",
       "         ...,\n",
       "         [-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654,  ..., -0.3088, -0.2303, -0.0039]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = (dim // n_heads) ** -0.5\n",
    "        self.qw = nn.Linear(dim, dim, bias = bias)\n",
    "        self.kw = nn.Linear(dim, dim, bias = bias)\n",
    "        self.vw = nn.Linear(dim, dim, bias = bias)\n",
    "\n",
    "        self.ow = nn.Linear(dim, dim, bias = bias)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        q = self.qw(x)\n",
    "        k = self.kw(x)\n",
    "        v = self.vw(x)\n",
    "\n",
    "        B, L, D = q.shape\n",
    "        q = torch.reshape(q, [B, L, self.n_heads, -1])\n",
    "        q = torch.permute(q, [0, 2, 1, 3])\n",
    "        k = torch.reshape(k, [B, L, self.n_heads, -1])\n",
    "        k = torch.permute(k, [0, 2, 3, 1])\n",
    "        v = torch.reshape(v, [B, L, self.n_heads, -1])\n",
    "        v = torch.permute(v, [0, 2, 1, 3])\n",
    "\n",
    "        qk = torch.matmul(q, k) * self.scale\n",
    "        qk = qk.masked_fill(self.bias[:,:,:L,:L] == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(qk, dim=-1)\n",
    "\n",
    "        v_attn = torch.matmul(attn, v)\n",
    "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
    "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
    "\n",
    "        x = self.ow(v_attn)\n",
    "        return x\n",
    "\n",
    "\n",
    "test_layer = Attention(32, maxlen, n_heads=1)\n",
    "test_layer(torch.ones([1, maxlen, 32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJmNuwH1clQd",
    "outputId": "ba31a20f-02ff-4a26-c8a4-c948d73708c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, maxlen, heads=4, mlp_dim=512, rate=0.0):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, maxlen)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(rate),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.ln_1(x)) + x\n",
    "        return self.mlp(self.ln_2(x)) + x\n",
    "\n",
    "\n",
    "test_layer = Transformer(32, maxlen)\n",
    "test_layer(torch.ones([1, maxlen, 32])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJaYcsx0ef58",
    "outputId": "6b939d94-d39b-4d29-fa79-ba23c1b18fcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 36])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2ndlTvDWsxf",
    "outputId": "af91749c-ac36-4670-fb41-d96e692e96ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 36, 38435]), torch.Size([128, 36]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, dim, vocab_size, maxlen, depth=3,\n",
    "                 mlp_dim=512, rate=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, maxlen, dim))\n",
    "\n",
    "        self.transformer = nn.Sequential()\n",
    "        for _ in range(depth):\n",
    "            self.transformer.append(Transformer(dim, maxlen))\n",
    "\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        x = self.embedding(x)\n",
    "        x += self.pos_embedding[:, :L]\n",
    "        x = self.transformer(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model_dim = 128\n",
    "depth = 3\n",
    "mlp_dim = 128\n",
    "\n",
    "gpt = GPT(dim=model_dim, vocab_size=spa_vocab_size,\n",
    "          maxlen=maxlen, depth=depth, mlp_dim=mlp_dim)\n",
    "output = gpt(train_batch)\n",
    "output.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsJbLyZ6b_57"
   },
   "source": [
    "## 4.- Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gxX1q5GrpE0",
    "outputId": "17fe9824-d631-4238-fe00-81a44f08fa78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(38435, 128)\n",
       "  (transformer): Sequential(\n",
       "    (0): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=128, out_features=38435, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etio6ZTwrpE0",
    "outputId": "70554a3e-7d2f-4866-b0c1-ca94c4a6818c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX = spa_vocab.get_stoi()['<pad>']\n",
    "PAD_IDX = spa_vocab.get_stoi()['<eos>']\n",
    "PAD_IDX, EOS_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ZyJNkKRkrpE1"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(gpt.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "R5bmAldOcJn0"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    start = time.time()\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        targets = targets.view(-1)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwUH9ZlHWsxh",
    "outputId": "596cd5e9-70b8-4b56-9b3f-a55d0b126c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i hate mondays . good-bye serruchando pírate kimono rajaran crueles ignorándote peacefully lavá luckily hundía subtitled regarding derretirse comparados traerías repite muertas vacant menciones they've thames capciosa non-members idiota apurado hazlo consulte peer tsunami collares 3:30 llegar elevar slid parecidos invitarnos molestia trasladas comía mandarín railways missing desplomado saldremos half-starved elevar aprobado escoltado vacia conocí changed obstruye estaciones gasté you'd vomited freezing logres escalando\n",
      "\n",
      "i love my cat . acomodar misjudged pica silverware piedras schoolyard monkeys demostrarlo royal amarnos soapy apartamentos divertimos maleducado plotting habituados well-dressed matrimonio frogs growth importaba axis dieron arguments hands modesto paseo outward retorcer : envíame easy-to-read revenge llamarle conveniencia somos molestia trasladas comía mandarín railways missing comunicarnos apegues consigna runners nada déjame x-rated descaro morderte amarré llaman marujees vibráfono strings fácil aprendí meager\n",
      "\n",
      "i like apples . good-bye serruchando pírate kimono rajaran crueles ignorándote peacefully lavá luckily hundía subtitled regarding derretirse comparados traerías revisemos quiera geneva valija cientos ordené calló impopular sentimiento votes apreciable grip interno siéntense afganistán construiste lloró maui lisa x sanded laboratorio fibra cruza farmers alfiler samurais derretido empezasteis blowing green perdónenos mentirte afírmate simplifica burlarte baseball pararan polleras mintiera belated presidencia autoridad estrepitosa\n"
     ]
    }
   ],
   "source": [
    "def translate(model, sentence, device, maxlen, vocab, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        idx = torch.tensor([vocab[token] for token in tokenizer(sentence)],\n",
    "                                    dtype=torch.long)\n",
    "        idx = idx.reshape([1, -1])\n",
    "        maxlen = maxlen - idx.shape[-1]\n",
    "\n",
    "        for _ in range(maxlen):\n",
    "            idx = idx.to(device)\n",
    "            logits = gpt(idx)[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        txt = ' '.join([vocab.get_itos()[_] for _ in idx[0]])\n",
    "\n",
    "        txt = txt.split(' < eos >')\n",
    "    return txt[0]\n",
    "\n",
    "sentences = ['i hate mondays.',\n",
    "             'i love my cat.',\n",
    "             'i like apples.']\n",
    "\n",
    "for s in sentences:\n",
    "    trans = translate(gpt, s, device, maxlen, spa_vocab, spa_tokenizer)\n",
    "    print(f\"\\n{trans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvsuJHMBlm9y",
    "outputId": "8e84f3d4-1690-438f-d1a3-44d35a5ed57a"
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 696.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Translate test sentences\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "Cell \u001b[0;32mIn[34], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/repos/NLP-GPT/env/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/repos/NLP-GPT/env/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/repos/NLP-GPT/env/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 696.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(gpt, device, train_loader, optimizer, epoch)\n",
    "\n",
    "    # Translate test sentences\n",
    "    for s in sentences:\n",
    "        trans = translate(gpt, s, device, maxlen, spa_vocab, spa_tokenizer)\n",
    "        print(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYVY8st5rpE3"
   },
   "source": [
    "## 5.- Evaluación (BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEQXDFwArpE3"
   },
   "outputs": [],
   "source": [
    "def bleu_example():\n",
    "    # Lista de oraciones de referencia (lista de listas)\n",
    "    referencias = [['El', 'gato', 'está', 'en', 'la', 'alfombra'],\n",
    "                   ['El', 'perro', 'juega', 'en', 'el', 'parque'],\n",
    "                   ['El', 'cielo', 'está', 'despejado'],\n",
    "                   ['El', 'sol', 'brilla', 'intensamente'],\n",
    "                   ['Los', 'pájaros', 'cantan', 'en', 'los', 'árboles']]\n",
    "\n",
    "    # Lista de oraciones candidatas (lista de listas)\n",
    "    candidatas = [['El', 'gato', 'está', 'durmiendo', 'en', 'la', 'alfombra'],\n",
    "                  ['El', 'perro', 'juega', 'en', 'el', 'jardín'],\n",
    "                  ['El', 'cielo', 'está', 'soleado'],\n",
    "                  ['El', 'sol', 'brilla', 'intensamente'],\n",
    "                  ['Los', 'pájaros', 'trinan', 'en', 'los', 'árboles']]\n",
    "\n",
    "    # Calcular el BLEU score para cada oración candidata\n",
    "    for i in range(len(candidatas)):\n",
    "        referencia = referencias[i]\n",
    "        candidata = candidatas[i]\n",
    "\n",
    "        bleu_score = nltk.translate.bleu_score.sentence_bleu([referencia], candidata)\n",
    "        print(f\"BLEU score para la oración {i+1}: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "VG2Dyu3grpE3",
    "outputId": "7e18da33-5eb7-44ed-a811-0eb8102739f2"
   },
   "outputs": [],
   "source": [
    "bleu_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8EiOMqLDeYr"
   },
   "outputs": [],
   "source": [
    "def format_string(s):\n",
    "  # Remove special characters\n",
    "  s = s.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
    "  # Delete multiple spaces\n",
    "  return ' '.join(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szj1creo-AC9",
    "outputId": "964667c8-04ca-41a1-bff1-1da389dab5ec"
   },
   "outputs": [],
   "source": [
    "def bleu_eval(test_data):\n",
    "\n",
    "  # Divide pairs in input/target\n",
    "  input = [s for s, _ in test_data]\n",
    "  target = [t for _, t in test_data]\n",
    "\n",
    "  # Get model outputs\n",
    "  output = []\n",
    "  for s in input:\n",
    "    trans = translate(gpt, s, device, maxlen, spa_vocab, spa_tokenizer)\n",
    "    output.append(trans)\n",
    "\n",
    "  # Delete multiple spaces and special characters\n",
    "  input = [format_string(s) for s in input]\n",
    "  target = [format_string(s) for s in target]\n",
    "  output = [format_string(s) for s in output]\n",
    "\n",
    "  # Delete input part from model output\n",
    "  for i in range(0, len(output)):\n",
    "    output[i] = output[i][len(input[i]) + 1:]\n",
    "\n",
    "  # Make list of lists for BLEU\n",
    "  target = [s.split() for s in target]\n",
    "  output = [s.split() for s in output]\n",
    "\n",
    "\n",
    "  # Compute BLEU\n",
    "  score = 0\n",
    "  for i in range(len(output)):\n",
    "    t = target[i]\n",
    "    o = output[i]\n",
    "\n",
    "    bleu_score = nltk.translate.bleu_score.sentence_bleu([t], o)\n",
    "    score += bleu_score\n",
    "\n",
    "  print(f'BLEU score promedio: {score/len(output)}')\n",
    "\n",
    "bleu_eval(test_pairs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
