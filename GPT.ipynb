{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NLyR9luRWsxI"
   },
   "outputs": [],
   "source": [
    "# The MIT License (MIT) Copyright (c) 2023 Emilio Morales\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in the Software without\n",
    "# restriction, including without limitation the rights to use, copy, modify, merge, publish,\n",
    "# distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or\n",
    "# substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n",
    "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
    "# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES\n",
    "# OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqusX_h4WsxK"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/milmor/NLP/blob/main/Notebooks/13_RNN_LSTM.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty7Z3RRm2v0P"
   },
   "source": [
    "# NLP Programa 3: GPT  \n",
    "-------\n",
    "Integrantes:\n",
    "- Andrés Urbano Andrea\n",
    "- Núñez Quintana Luis Axel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.- Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 5\n",
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 20:51:42.077773: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-27 20:51:43.183912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/axel/Desktop/repos/NLP-GPT/env/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/axel/Desktop/repos/NLP-GPT/env/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/axel/Desktop/repos/NLP-GPT/env/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import keras_core as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x76ce74adf8b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "torch.manual_seed(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuZuE99sWsxO"
   },
   "source": [
    "## 1.- Conjuntos de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnlt2x--WsxV",
    "outputId": "570a1fc8-fc10-44ce-e06e-a23a7c88dca5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_text_pairs():\n",
    "    path_to_zip = tf.keras.utils.get_file(\n",
    "        'spa-eng.zip', \n",
    "        origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "        extract=True)\n",
    "    path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'\n",
    "    \n",
    "    with open(path_to_file) as f:\n",
    "        lines = f.read().split(\"\\n\")[:-1]\n",
    "        \n",
    "    text_pairs = []\n",
    "    for line in lines:\n",
    "        eng, spa = line.lower().split(\"\\t\")\n",
    "        text_pairs.append((eng, spa))\n",
    "    return text_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_pairs(text_pairs, val_percentage = 0.005, random_seed=43):\n",
    "    random.Random(random_seed).shuffle(text_pairs)\n",
    "    num_val_samples = int(val_percentage * len(text_pairs))\n",
    "    num_train_samples = len(text_pairs) - num_val_samples\n",
    "    train_pairs = text_pairs[:num_train_samples]\n",
    "    val_pairs = text_pairs[num_train_samples:]\n",
    "    return train_pairs, val_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "118370 training pairs\n",
      "594 validation pairs\n",
      "('the old woman fell and could not get up.', 'la anciana se cayó y no pudo levantarse.')\n",
      "('what is this the abbreviation for?', '¿de qué es abreviatura esto?')\n",
      "(\"you're not sick.\", 'no estás enferma.')\n"
     ]
    }
   ],
   "source": [
    "text_pairs = download_text_pairs()\n",
    "train_pairs, val_pairs = split_text_pairs(text_pairs)\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "\n",
    "for s in train_pairs[:3]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4KjIrLz8Ryf",
    "outputId": "f4ace4f8-ec84-4556-cba5-87748562de3c"
   },
   "source": [
    "## 2.- Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWW4RxWhpAZf"
   },
   "source": [
    "- Crea vocabulario y define tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0viwLWSUpGrk"
   },
   "outputs": [],
   "source": [
    "eng_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "spa_tokenizer = get_tokenizer('spacy', language='es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qpTS_ktRo_xd"
   },
   "outputs": [],
   "source": [
    "def build_vocab(text, tokenizers, min_freq=5):\n",
    "    eng_tokenizer, spa_tokenizer = tokenizers\n",
    "    eng_counter = Counter()\n",
    "    spa_counter = Counter()\n",
    "    for eng_string_, spa_string_ in text:\n",
    "        eng_counter.update(eng_tokenizer(eng_string_))\n",
    "        spa_counter.update(spa_tokenizer(spa_string_))\n",
    "    eng_vocab = Vocab(eng_counter, min_freq=min_freq, \n",
    "                       specials=['<unk>', '<pad>'])\n",
    "    spa_vocab = Vocab(spa_counter, min_freq=min_freq, \n",
    "                       specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "    return eng_vocab, spa_vocab\n",
    "\n",
    "\n",
    "eng_vocab, spa_vocab = build_vocab(text_pairs, \n",
    "                                   [eng_tokenizer, spa_tokenizer],\n",
    "                                   min_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlV0maHzt88t",
    "outputId": "df3f7b6e-9058-4a87-ffc7-45a360d2bdeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab sizes: English - 13229, Spanish - 26116\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_vocab)\n",
    "spa_vocab_size = len(spa_vocab)\n",
    "print(f'Vocab sizes: English - {eng_vocab_size}, Spanish - {spa_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Og1BFvJrWsxa",
    "outputId": "9dd04a1e-b7b3-4742-8303-7ee5f0e058fd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxlen = 10\n",
    "\n",
    "def data_process(text):\n",
    "    data = []\n",
    "    for eng, spa in text:\n",
    "        eng_tensor_ = torch.tensor([eng_vocab[token] for token in eng_tokenizer(eng)],\n",
    "                                dtype=torch.long)\n",
    "        spa_tensor_ = torch.tensor([spa_vocab[token] for token in spa_tokenizer(spa)],\n",
    "                                dtype=torch.long)\n",
    "\n",
    "        if eng_tensor_.shape[0] < maxlen:\n",
    "            data.append((eng_tensor_, spa_tensor_))\n",
    "    return data\n",
    "\n",
    "train_data = data_process(train_pairs)\n",
    "val_data = data_process(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 93861, val data size: 481\n"
     ]
    }
   ],
   "source": [
    "print(f'train data size: {len(train_data)}, val data size: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cW6E9jkJTe1o",
    "outputId": "c9b814fe-b035-4379-f7a7-f69e65317894"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "PAD_IDX = eng_vocab['<pad>']\n",
    "BOS_IDX = spa_vocab['<bos>']\n",
    "EOS_IDX = spa_vocab['<eos>']\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    x, y = [], []\n",
    "    for (x_item, y_item) in data_batch:\n",
    "        x.append(x_item)\n",
    "        y.append(torch.cat([torch.tensor([BOS_IDX]), \n",
    "                            y_item, \n",
    "                            torch.tensor([EOS_IDX])], dim=0))\n",
    "\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
    "    y = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
    "                          shuffle=True, collate_fn=generate_batch, \n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch,\n",
    "                        num_workers=4, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(val_data, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch,\n",
    "                        num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_N6IRYsT4ry",
    "outputId": "35681e3f-5639-4b76-f7cb-32da7cb6cff4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.36 s ± 87.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aauQX2tFXD8K"
   },
   "outputs": [],
   "source": [
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROw_9EjsT2Et",
    "outputId": "f5152bf2-ab07-43f8-f544-085ba9d58e23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 9]), torch.Size([64, 12]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 21,  50,  36, 678,  98, 436,  27, 161,  11])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ7f4DHJreIj"
   },
   "source": [
    "## 3.- Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "model_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRwHxAuGYEKa",
    "outputId": "eb3a33d0-b56f-4c86-8c66-ffba3209fc82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3488, -0.3941, -0.1654, -0.0524,  0.3346,  0.0233, -0.7930,\n",
       "           0.4600, -0.1046, -0.2694,  0.3566,  0.2376,  0.5142,  0.2729,\n",
       "           0.1786, -0.1451,  0.1851,  0.7246, -0.5706,  0.2467,  0.5526,\n",
       "           0.7210, -0.1349,  0.2598,  0.1340, -0.2740,  0.6389, -0.1100,\n",
       "          -0.4303, -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654, -0.0524,  0.3346,  0.0233, -0.7930,\n",
       "           0.4600, -0.1046, -0.2694,  0.3566,  0.2376,  0.5142,  0.2729,\n",
       "           0.1786, -0.1451,  0.1851,  0.7246, -0.5706,  0.2467,  0.5526,\n",
       "           0.7210, -0.1349,  0.2598,  0.1340, -0.2740,  0.6389, -0.1100,\n",
       "          -0.4303, -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654, -0.0524,  0.3346,  0.0233, -0.7930,\n",
       "           0.4600, -0.1046, -0.2694,  0.3566,  0.2376,  0.5142,  0.2729,\n",
       "           0.1786, -0.1451,  0.1851,  0.7246, -0.5706,  0.2467,  0.5526,\n",
       "           0.7210, -0.1349,  0.2598,  0.1340, -0.2740,  0.6389, -0.1100,\n",
       "          -0.4303, -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654, -0.0524,  0.3346,  0.0233, -0.7930,\n",
       "           0.4600, -0.1046, -0.2694,  0.3566,  0.2376,  0.5142,  0.2729,\n",
       "           0.1786, -0.1451,  0.1851,  0.7246, -0.5706,  0.2467,  0.5526,\n",
       "           0.7210, -0.1349,  0.2598,  0.1340, -0.2740,  0.6389, -0.1100,\n",
       "          -0.4303, -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654, -0.0524,  0.3346,  0.0233, -0.7930,\n",
       "           0.4600, -0.1046, -0.2694,  0.3566,  0.2376,  0.5142,  0.2729,\n",
       "           0.1786, -0.1451,  0.1851,  0.7246, -0.5706,  0.2467,  0.5526,\n",
       "           0.7210, -0.1349,  0.2598,  0.1340, -0.2740,  0.6389, -0.1100,\n",
       "          -0.4303, -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654, -0.0524,  0.3346,  0.0233, -0.7930,\n",
       "           0.4600, -0.1046, -0.2694,  0.3566,  0.2376,  0.5142,  0.2729,\n",
       "           0.1786, -0.1451,  0.1851,  0.7246, -0.5706,  0.2467,  0.5526,\n",
       "           0.7210, -0.1349,  0.2598,  0.1340, -0.2740,  0.6389, -0.1100,\n",
       "          -0.4303, -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654, -0.0524,  0.3346,  0.0233, -0.7930,\n",
       "           0.4600, -0.1046, -0.2694,  0.3566,  0.2376,  0.5142,  0.2729,\n",
       "           0.1786, -0.1451,  0.1851,  0.7246, -0.5706,  0.2467,  0.5526,\n",
       "           0.7210, -0.1349,  0.2598,  0.1340, -0.2740,  0.6389, -0.1100,\n",
       "          -0.4303, -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654, -0.0524,  0.3346,  0.0233, -0.7930,\n",
       "           0.4600, -0.1046, -0.2694,  0.3566,  0.2376,  0.5142,  0.2729,\n",
       "           0.1786, -0.1451,  0.1851,  0.7246, -0.5706,  0.2467,  0.5526,\n",
       "           0.7210, -0.1349,  0.2598,  0.1340, -0.2740,  0.6389, -0.1100,\n",
       "          -0.4303, -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3488, -0.3941, -0.1654, -0.0524,  0.3346,  0.0233, -0.7930,\n",
       "           0.4600, -0.1046, -0.2694,  0.3566,  0.2376,  0.5142,  0.2729,\n",
       "           0.1786, -0.1451,  0.1851,  0.7246, -0.5706,  0.2467,  0.5526,\n",
       "           0.7210, -0.1349,  0.2598,  0.1340, -0.2740,  0.6389, -0.1100,\n",
       "          -0.4303, -0.3088, -0.2303, -0.0039],\n",
       "         [-0.3489, -0.3941, -0.1654, -0.0524,  0.3346,  0.0233, -0.7930,\n",
       "           0.4600, -0.1046, -0.2694,  0.3566,  0.2376,  0.5142,  0.2729,\n",
       "           0.1786, -0.1451,  0.1851,  0.7246, -0.5706,  0.2467,  0.5526,\n",
       "           0.7210, -0.1349,  0.2598,  0.1340, -0.2740,  0.6389, -0.1100,\n",
       "          -0.4303, -0.3088, -0.2303, -0.0039]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = (dim // n_heads) ** -0.5\n",
    "        self.qw = nn.Linear(dim, dim, bias = bias)\n",
    "        self.kw = nn.Linear(dim, dim, bias = bias)\n",
    "        self.vw = nn.Linear(dim, dim, bias = bias)\n",
    "\n",
    "        self.ow = nn.Linear(dim, dim, bias = bias)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        q = self.qw(x)\n",
    "        k = self.kw(x)\n",
    "        v = self.vw(x)\n",
    "\n",
    "        B, L, D = q.shape\n",
    "        q = torch.reshape(q, [B, L, self.n_heads, -1])\n",
    "        q = torch.permute(q, [0, 2, 1, 3])\n",
    "        k = torch.reshape(k, [B, L, self.n_heads, -1])\n",
    "        k = torch.permute(k, [0, 2, 3, 1])\n",
    "        v = torch.reshape(v, [B, L, self.n_heads, -1])\n",
    "        v = torch.permute(v, [0, 2, 1, 3])\n",
    "\n",
    "        qk = torch.matmul(q, k) * self.scale\n",
    "        qk = qk.masked_fill(self.bias[:,:,:L,:L] == 0, float('-inf'))\n",
    "        \n",
    "        attn = torch.softmax(qk, dim=-1)\n",
    "\n",
    "        v_attn = torch.matmul(attn, v)\n",
    "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
    "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
    "\n",
    "        x = self.ow(v_attn)\n",
    "        return x\n",
    "\n",
    "\n",
    "test_layer = Attention(32, maxlen, n_heads=1)\n",
    "test_layer(torch.ones([1, maxlen, 32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJmNuwH1clQd",
    "outputId": "806ce157-d0b0-4343-d069-2825e02feb09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, maxlen, heads=4, mlp_dim=512, rate=0.0):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, maxlen)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(rate),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.ln_1(x)) + x\n",
    "        return self.mlp(self.ln_2(x)) + x\n",
    "\n",
    "\n",
    "test_layer = Transformer(32, maxlen)\n",
    "test_layer(torch.ones([1, maxlen, 32])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJaYcsx0ef58",
    "outputId": "90d36f9f-e146-4d80-a9ba-ab4a6d12e7db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 9])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2ndlTvDWsxf",
    "outputId": "7fdf146c-9fef-4a86-d7cf-356489df232a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 9, 39345]), torch.Size([64, 12]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, dim, vocab_size, maxlen, depth=3, \n",
    "                 mlp_dim=512, rate=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, maxlen, dim))\n",
    "\n",
    "        self.transformer = nn.Sequential()\n",
    "        for _ in range(depth):\n",
    "            self.transformer.append(Transformer(dim, maxlen))\n",
    "\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        x = self.embedding(x)\n",
    "        x += self.pos_embedding[:, :L]\n",
    "        x = self.transformer(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "model_dim = 128\n",
    "depth = 3\n",
    "mlp_dim = 128\n",
    "\n",
    "gpt = GPT(dim=model_dim, vocab_size=eng_vocab_size + spa_vocab_size, \n",
    "          maxlen=maxlen, depth=depth, mlp_dim=mlp_dim)\n",
    "output = gpt(train_batch)\n",
    "output.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsJbLyZ6b_57"
   },
   "source": [
    "## 4.- Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(39345, 128)\n",
       "  (transformer): Sequential(\n",
       "    (0): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=128, out_features=39345, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(gpt.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "R5bmAldOcJn0"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    start = time.time()\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        targets = targets.view(-1)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xwUH9ZlHWsxh"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslate spanish to english me encantan los perros\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslate spanish to english me gusta dormir\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslate spanish to english el gato come manzanas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m---> 27\u001b[0m     trans \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(model, sentence, device, maxlen)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m     idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([vocab[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenizer\u001b[49m(sentence)],\n\u001b[1;32m      5\u001b[0m                                 dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      6\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      7\u001b[0m     maxlen \u001b[38;5;241m=\u001b[39m maxlen \u001b[38;5;241m-\u001b[39m idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def translate(model, sentence, device, maxlen):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        idx = torch.tensor([vocab[token] for token in tokenizer(sentence)],\n",
    "                                    dtype=torch.long)\n",
    "        idx = idx.reshape([1, -1])\n",
    "        maxlen = maxlen - idx.shape[-1]\n",
    "\n",
    "        for _ in range(maxlen):\n",
    "            idx = idx.to(device)\n",
    "            logits = gpt(idx)[:, -1, :]      \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        txt = \" \".join(\n",
    "                    [vocab.get_itos()[idx[0, _]] for _ in range(maxlen)]\n",
    "                )\n",
    "    return txt.replace(\"<eos>\", \"\")\n",
    "        \n",
    "sentences = ['translate spanish to english me encantan los perros',\n",
    "             'translate spanish to english me gusta dormir',\n",
    "             'translate spanish to english el gato come manzanas']\n",
    "\n",
    "for s in sentences:\n",
    "    trans = translate(gpt, s, device, maxlen)\n",
    "    print(f\"\\n{trans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvsuJHMBlm9y",
    "outputId": "4e6da109-4c05-4629-c327-a37fbc684b9e"
   },
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(gpt, device, train_loader, optimizer, epoch)\n",
    "    \n",
    "    # Translate test sentences\n",
    "    for s in sentences:\n",
    "        trans = translate(gpt, s, device, maxlen)\n",
    "        print(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Evaluación (BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_example():\n",
    "    # Lista de oraciones de referencia (lista de listas)\n",
    "    referencias = [['El', 'gato', 'está', 'en', 'la', 'alfombra'],\n",
    "                   ['El', 'perro', 'juega', 'en', 'el', 'parque'],\n",
    "                   ['El', 'cielo', 'está', 'despejado'],\n",
    "                   ['El', 'sol', 'brilla', 'intensamente'],\n",
    "                   ['Los', 'pájaros', 'cantan', 'en', 'los', 'árboles']]\n",
    "    \n",
    "    # Lista de oraciones candidatas (lista de listas)\n",
    "    candidatas = [['El', 'gato', 'está', 'durmiendo', 'en', 'la', 'alfombra'],\n",
    "                  ['El', 'perro', 'juega', 'en', 'el', 'jardín'],\n",
    "                  ['El', 'cielo', 'está', 'soleado'],\n",
    "                  ['El', 'sol', 'brilla', 'intensamente'],\n",
    "                  ['Los', 'pájaros', 'trinan', 'en', 'los', 'árboles']]\n",
    "    \n",
    "    # Calcular el BLEU score para cada oración candidata\n",
    "    for i in range(len(candidatas)):\n",
    "        referencia = referencias[i]\n",
    "        candidata = candidatas[i]\n",
    "        \n",
    "        bleu_score = nltk.translate.bleu_score.sentence_bleu([referencia], candidata)\n",
    "        print(f\"BLEU score para la oración {i+1}: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_example()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
